{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load, sentence split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_preprocess_utils import load_jsonl,split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = load_jsonl('../data/preprocess_test_data.jsonl')\n",
    "print(f\"original_data, size = {len(load_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sentences = split_sentences(load_data)\n",
    "print(\"splitted sentence: \\n\")\n",
    "for i in range(0, len(load_data)):\n",
    "     print(f\"multi-doc samples {i + 1} has {len(doc_sentences[i])} docs.\")\n",
    "     for doc in doc_sentences[i]:\n",
    "          print(f\"sentence number: {len(doc)}\")\n",
    "          print(f\"document sentences: {doc}\")\n",
    "\n",
    "     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keyword extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_preprocess_utils import extract_keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "doc = \"\"\"\n",
    "          Supervised learning is the machine learning task of learning a function that\n",
    "          maps an input to an output based on example input-output pairs.[1] It infers a\n",
    "          function from labeled training data consisting of a set of training examples.[2]\n",
    "          In supervised learning, each example is a pair consisting of an input object\n",
    "          (typically a vector) and a desired output value (also called the supervisory signal).\n",
    "          A supervised learning algorithm analyzes the training data and produces an inferred function,\n",
    "          which can be used for mapping new examples. An optimal scenario will allow for the\n",
    "          algorithm to correctly determine the class labels for unseen instances. This requires\n",
    "          the learning algorithm to generalize from the training data to unseen situations in a\n",
    "          'reasonable' way (see inductive bias).\n",
    "          \"\"\"\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence keyword\n",
    "s1 = \"Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.\"\n",
    "s2 = \"I am a women.\"\n",
    "s3 = \"hei!\"\n",
    "kw1 = kw_model.extract_keywords(s1)\n",
    "kw2 = kw_model.extract_keywords(s2)\n",
    "kw3 = kw_model.extract_keywords(s3)\n",
    "\n",
    "print(f\"sentence keywords: \\n kw1: {kw1} \\n kw2: {kw2} \\n kw3: {kw3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stop word remove\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# 获取 spaCy 的停用词列表\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "kw1_noStopWord = [(word, score) for word, score in kw1 if word.lower() not in stop_words]\n",
    "kw2_noStopWord = [(word, score) for word, score in kw2 if word.lower() not in stop_words]\n",
    "kw3_noStopWord = [(word, score) for word, score in kw3 if word.lower() not in stop_words]\n",
    "print(f\"sentence keywords without stopwords: \\n kw1: {kw1_noStopWord} \\n kw2: {kw2_noStopWord} \\n kw3: {kw3_noStopWord}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_keyword_count(text, words_per_100=1, min_keywords=2, max_keywords=1100):\n",
    "     # 计算文本长度\n",
    "     text_length = len(text)\n",
    "     print(text_length)\n",
    "     # 按比例计算关键词数量\n",
    "     keyword_count = max(min_keywords, min(max_keywords, text_length // 4 * words_per_100))\n",
    "     \n",
    "     return keyword_count\n",
    "\n",
    "# 示例\n",
    "text = \"这是一段示例文本，用于演示如何根据文本长度动态调整关键词数量。\"\n",
    "keyword_count = calculate_keyword_count(text)\n",
    "print(f\"建议提取的关键词数量: {keyword_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coreference resolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new method try\n",
    "import coreferee\n",
    "import spacy\n",
    "\n",
    "nlp_l = spacy.load('en_core_web_lg')\n",
    "nlp_l.add_pipe('coreferee')\n",
    "doc = nlp_l('Although he was very busy with his work, Peter had had enough of it. He and his wife decided they needed a holiday. They travelled to Spain because they loved the country very much. She also meet Tom, Anny and John, who are their previous colleges')\n",
    "# print(type(doc))\n",
    "doc._.coref_chains.print()\n",
    "# print(type(doc._.coref_chains[0]))\n",
    "\n",
    "# for chain in doc._.coref_chains:\n",
    "#      print(f\"current chain: {chain}. \\n its mentions are: \")\n",
    "#      for mention in chain:\n",
    "#           print(mention)\n",
    "          \n",
    "# print(doc._.coref_chains.resolve(doc[31]))\n",
    "\n",
    "# print(doc._.coref_chains[0].pretty_representation)\n",
    "ind = doc._.coref_chains[5][1][0]\n",
    "print(f\"index: {ind}, ancetedent: {doc._.coref_chains.resolve(doc[ind])}\") #  ancetedent: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = doc._.coref_chains[0]\n",
    "print(f\"chain 1 antecend: {chain.most_specific_mention_index}, token_id:{chain[chain.most_specific_mention_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{doc[34]} is in the sent: {doc[34].sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## over-all test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\coreferee_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.data_preprocess_utils import define_node_edge, extract_keywords, coref_resolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs_list = [[\n",
    "     [\"Although he was very busy with his work, Peter had had enough of it. He and his wife decided they needed a holiday. They travelled to Spain because they loved the country very much.\"],\n",
    "     [\"A stray cat appeared in the garden one rainy day. Sarah offered milk and a blanket. It stayed, becoming part of her family.\"],\n",
    "     [\"The bakery’s croissants were legendary. Every morning, the scent of butter drew a crowd. Mrs. Laurent, the baker, perfected each one. Customers claimed they were the best they’d ever tasted.\"]\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_node_list, sent_node_list, edge_data_list, sentId_nodeId_list = define_node_edge(test_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'peter': 10,\n",
       " 'spain': 11,\n",
       " 'stray': 12,\n",
       " 'milk': 13,\n",
       " 'croissants': 14,\n",
       " 'laurent': 15}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_node_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {(10, 0): [{'type': 'word_sent', 'weight': 0.4611}],\n",
       "             (11, 2): [{'type': 'word_sent', 'weight': 0.2796}],\n",
       "             (12, 3): [{'type': 'word_sent', 'weight': 0.4608}],\n",
       "             (13, 4): [{'type': 'word_sent', 'weight': 0.1903}],\n",
       "             (14, 6): [{'type': 'word_sent', 'weight': 0.6856}],\n",
       "             (15, 8): [{'type': 'word_sent', 'weight': 0.284}],\n",
       "             (1, 0): [{'type': 'pronoun_antecedent', 'weight': 1},\n",
       "              {'type': 'similarity', 'weight': tensor(0.3666)}],\n",
       "             (2, 1): [{'type': 'pronoun_antecedent', 'weight': 1},\n",
       "              {'type': 'similarity', 'weight': tensor(0.3891)}],\n",
       "             (5, 3): [{'type': 'pronoun_antecedent', 'weight': 1}],\n",
       "             (5, 4): [{'type': 'pronoun_antecedent', 'weight': 1}],\n",
       "             (8, 6): [{'type': 'similarity', 'weight': tensor(0.4890)}],\n",
       "             (9, 6): [{'type': 'similarity', 'weight': tensor(0.3625)}]})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### doc1\n",
    "'''\n",
    "Although he was very busy with his work, Peter had had enough of it. n1,sen1\n",
    "He and his wife decided they needed a holiday. n2,sen2\n",
    "They travelled to Spain because they loved the country very much. n3,sen3\n",
    "'''\n",
    "####\n",
    "#### doc2\n",
    "'''\n",
    "A stray cat appeared in the garden one rainy day. n4,sen1\n",
    "Sarah offered milk and a blanket. n5,sen2\n",
    "It stayed, becoming part of her family. n6,sen3\n",
    "'''\n",
    "####\n",
    "#### doc3\n",
    "'''\n",
    "The bakery’s croissants were legendary. n7,sen1\n",
    "Every morning, the scent of butter drew a crowd. n8,sen2\n",
    "Mrs. Laurent, the baker, perfected each one. n9,sen3\n",
    "Customers claimed they were the best they’d ever tasted. n10,sen4\n",
    "'''\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_kws = extract_keywords(test_docs_list)\n",
    "\n",
    "for docs in docs_kws:\n",
    "     for doc in docs:\n",
    "          print(\"final result: \", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corfs = coref_resolve(test_docs_list)\n",
    "\n",
    "for docs in corfs:\n",
    "     for doc in docs:\n",
    "          print(\"doc final result: \")\n",
    "          for cluster in doc:\n",
    "               print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_map, sent_map, edges = define_node_edge(test_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"word map: \", word_map)\n",
    "print(\"sent_map: \", sent_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in word_map[0].items():\n",
    "     print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (k1,k2), value in edges[0].items():\n",
    "     for e in value:\n",
    "          print(f\"{k1}, {k2}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 3.83k/3.83k [00:00<?, ?B/s]\n",
      "Downloading readme: 100%|██████████| 10.6k/10.6k [00:00<00:00, 2.35MB/s]\n",
      "Downloading data: 100%|██████████| 548M/548M [06:21<00:00, 1.43MB/s]\n",
      "Downloading data: 100%|██████████| 58.8M/58.8M [00:34<00:00, 1.73MB/s]\n",
      "Downloading data: 100%|██████████| 66.9M/66.9M [00:39<00:00, 1.70MB/s]\n",
      "Downloading data: 100%|██████████| 7.30M/7.30M [00:04<00:00, 1.59MB/s]\n",
      "Downloading data: 100%|██████████| 69.0M/69.0M [00:43<00:00, 1.60MB/s]\n",
      "Downloading data: 100%|██████████| 7.31M/7.31M [00:04<00:00, 1.48MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [08:51<00:00, 177.25s/it]\n",
      "Generating train split: 100%|██████████| 44972/44972 [00:11<00:00, 3769.23 examples/s]\n",
      "Generating validation split: 100%|██████████| 5622/5622 [00:01<00:00, 4062.35 examples/s]\n",
      "Generating test split: 100%|██████████| 5622/5622 [00:01<00:00, 3790.77 examples/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Loading a dataset cached in a LocalFileSystem is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malexfabbri/multi_news\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\coreferee_env\\lib\\site-packages\\datasets\\load.py:2149\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   2145\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   2146\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2147\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   2148\u001b[0m )\n\u001b[1;32m-> 2149\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2150\u001b[0m \u001b[38;5;66;03m# Rename and cast features to match task schema\u001b[39;00m\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2152\u001b[0m     \u001b[38;5;66;03m# To avoid issuing the same warning twice\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\coreferee_env\\lib\\site-packages\\datasets\\builder.py:1173\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[1;34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[0m\n\u001b[0;32m   1171\u001b[0m is_local \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_filesystem(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs)\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local:\n\u001b[1;32m-> 1173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a dataset cached in a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir):\n\u001b[0;32m   1175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1176\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: could not find data in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please make sure to call \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilder.download_and_prepare(), or use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1178\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets.load_dataset() before trying to access the Dataset object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1179\u001b[0m     )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Loading a dataset cached in a LocalFileSystem is not supported."
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"alexfabbri/multi_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(samples):\n",
    "     ori_docs = [[doc] for doc in samples['document']]\n",
    "     summary_list = samples['summary']\n",
    "     doc_list = [doc[0].split('|||||') for doc in ori_docs]\n",
    "     converted_list = [[[item] for item in sublist] for sublist in doc_list]\n",
    "     \n",
    "     return  {\"text\": converted_list, \"summary\": summary_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 25.03 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['document', 'summary', 'text'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "a = {\n",
    "     \"document\": [\n",
    "     \"National Archives \\n\\nYes, it’s that time again, folks at 8:30 a.m. ||||| Employers pulled back sharply on hiring last month\",\n",
    "     \"LOS ANGELES (AP) —  ||||| Shelly Sterling said today that 'eventually'\"\n",
    "     ],\n",
    "     \"summary\": [\n",
    "     \"– The unemployment rate dropped to 8.2% last month, but the economy only added 120,000 jobs, when 203,000 new jobs had been predicted, according to today's jobs report. .\",\n",
    "     \"– Shelly Sterling plans 'eventually' to divorce her estranged husband Donald'\"\n",
    "     ]\n",
    "}\n",
    "b = Dataset.from_dict(a)\n",
    "c = b.map(preprocess_function, batched=True)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 'National Archives \\n\\nYes, it’s that time again, folks at 8:30 a.m. ||||| Employers pulled back sharply on hiring last month',\n",
       " 'summary': \"– The unemployment rate dropped to 8.2% last month, but the economy only added 120,000 jobs, when 203,000 new jobs had been predicted, according to today's jobs report. .\",\n",
       " 'text': [['National Archives \\n\\nYes, it’s that time again, folks at 8:30 a.m. '],\n",
       "  [' Employers pulled back sharply on hiring last month']]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"sample.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "     for sample in c:\n",
    "          json_line = json.dumps(sample, ensure_ascii=False)\n",
    "          # 写入文件\n",
    "          f.write(json_line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_jsonl(data, filename):\n",
    "     with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "          json_line = json.dumps(data, ensure_ascii=False)\n",
    "          f.write(json_line + \"\\n\")\n",
    "               \n",
    "save_to_jsonl(c, \"test_out.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分别处理三个数据集\n",
    "train_data = dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "val_data = dataset[\"validation\"].map(preprocess_function, batched=True)\n",
    "test_data = dataset[\"test\"].map(preprocess_function, batched=True)\n",
    "\n",
    "# 保存为 .jsonl 文件\n",
    "def save_to_jsonl(data, filename):\n",
    "     with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "          for example in data:\n",
    "               json_line = json.dumps(example, ensure_ascii=False)\n",
    "               f.write(json_line + \"\\n\")\n",
    "\n",
    "file_path_prefix  = '../data/multinews_'\n",
    "# 保存训练集\n",
    "save_to_jsonl(train_data, file_path_prefix + \"train.jsonl\")\n",
    "\n",
    "# 保存验证集\n",
    "save_to_jsonl(val_data, file_path_prefix + \"validation.jsonl\")\n",
    "\n",
    "# 保存测试集\n",
    "save_to_jsonl(test_data, file_path_prefix + \"test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'safsfd\\'cv\\'dsfs\"Sdga\"'\n"
     ]
    }
   ],
   "source": [
    "raw_string = 'safsfd\\'cv\\'dsfs\"Sdga\"'\n",
    "escaped = repr(raw_string)\n",
    "print(escaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"safsfd\\'cv\\'dsfs\\\\\"Sdga\\\\\"\"'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(raw_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coreferee_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
