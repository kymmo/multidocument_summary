{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load, sentence split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_preprocess_utils import load_jsonl,split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = load_jsonl('../data/preprocess_test_data.jsonl')\n",
    "print(f\"original_data, size = {len(load_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sentences = split_sentences(load_data)\n",
    "print(\"splitted sentence: \\n\")\n",
    "for i in range(0, len(load_data)):\n",
    "     print(f\"multi-doc samples {i + 1} has {len(doc_sentences[i])} docs.\")\n",
    "     for doc in doc_sentences[i]:\n",
    "          print(f\"sentence number: {len(doc)}\")\n",
    "          print(f\"document sentences: {doc}\")\n",
    "\n",
    "     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keyword extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_preprocess_utils import extract_keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "doc = \"\"\"\n",
    "          Supervised learning is the machine learning task of learning a function that\n",
    "          maps an input to an output based on example input-output pairs.[1] It infers a\n",
    "          function from labeled training data consisting of a set of training examples.[2]\n",
    "          In supervised learning, each example is a pair consisting of an input object\n",
    "          (typically a vector) and a desired output value (also called the supervisory signal).\n",
    "          A supervised learning algorithm analyzes the training data and produces an inferred function,\n",
    "          which can be used for mapping new examples. An optimal scenario will allow for the\n",
    "          algorithm to correctly determine the class labels for unseen instances. This requires\n",
    "          the learning algorithm to generalize from the training data to unseen situations in a\n",
    "          'reasonable' way (see inductive bias).\n",
    "          \"\"\"\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence keyword\n",
    "s1 = \"Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.\"\n",
    "s2 = \"I am a women.\"\n",
    "s3 = \"hei!\"\n",
    "kw1 = kw_model.extract_keywords(s1)\n",
    "kw2 = kw_model.extract_keywords(s2)\n",
    "kw3 = kw_model.extract_keywords(s3)\n",
    "\n",
    "print(f\"sentence keywords: \\n kw1: {kw1} \\n kw2: {kw2} \\n kw3: {kw3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stop word remove\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# 获取 spaCy 的停用词列表\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "kw1_noStopWord = [(word, score) for word, score in kw1 if word.lower() not in stop_words]\n",
    "kw2_noStopWord = [(word, score) for word, score in kw2 if word.lower() not in stop_words]\n",
    "kw3_noStopWord = [(word, score) for word, score in kw3 if word.lower() not in stop_words]\n",
    "print(f\"sentence keywords without stopwords: \\n kw1: {kw1_noStopWord} \\n kw2: {kw2_noStopWord} \\n kw3: {kw3_noStopWord}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_keyword_count(text, words_per_100=1, min_keywords=2, max_keywords=1100):\n",
    "     # 计算文本长度\n",
    "     text_length = len(text)\n",
    "     print(text_length)\n",
    "     # 按比例计算关键词数量\n",
    "     keyword_count = max(min_keywords, min(max_keywords, text_length // 4 * words_per_100))\n",
    "     \n",
    "     return keyword_count\n",
    "\n",
    "# 示例\n",
    "text = \"这是一段示例文本，用于演示如何根据文本长度动态调整关键词数量。\"\n",
    "keyword_count = calculate_keyword_count(text)\n",
    "print(f\"建议提取的关键词数量: {keyword_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coreference resolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new method try\n",
    "import coreferee\n",
    "import spacy\n",
    "\n",
    "nlp_l = spacy.load('en_core_web_lg')\n",
    "nlp_l.add_pipe('coreferee')\n",
    "doc = nlp_l('Although he was very busy with his work, Peter had had enough of it. He and his wife decided they needed a holiday. They travelled to Spain because they loved the country very much. She also meet Tom, Anny and John, who are their previous colleges')\n",
    "# print(type(doc))\n",
    "doc._.coref_chains.print()\n",
    "# print(type(doc._.coref_chains[0]))\n",
    "\n",
    "# for chain in doc._.coref_chains:\n",
    "#      print(f\"current chain: {chain}. \\n its mentions are: \")\n",
    "#      for mention in chain:\n",
    "#           print(mention)\n",
    "          \n",
    "# print(doc._.coref_chains.resolve(doc[31]))\n",
    "\n",
    "# print(doc._.coref_chains[0].pretty_representation)\n",
    "ind = doc._.coref_chains[5][1][0]\n",
    "print(f\"index: {ind}, ancetedent: {doc._.coref_chains.resolve(doc[ind])}\") #  ancetedent: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = doc._.coref_chains[0]\n",
    "print(f\"chain 1 antecend: {chain.most_specific_mention_index}, token_id:{chain[chain.most_specific_mention_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{doc[34]} is in the sent: {doc[34].sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## over-all test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.data_preprocess_utils import define_node_edge, extract_keywords, coref_resolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 19:32:06,782 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu - SentenceTransformer.py:210\n",
      "2025-01-23 19:32:06,782 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2 - SentenceTransformer.py:218\n",
      "2025-01-23 19:32:13,860 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu - SentenceTransformer.py:210\n",
      "2025-01-23 19:32:13,860 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2 - SentenceTransformer.py:218\n"
     ]
    }
   ],
   "source": [
    "test_docs_list = [[\n",
    "     [\"Although he was very busy with his work, Peter had had enough of it. He and his wife decided they needed a holiday. They travelled to Spain because they loved the country very much.\"],\n",
    "     [\"A stray cat appeared in the garden one rainy day. Sarah offered milk and a blanket. It stayed, becoming part of her family.\"],\n",
    "     [\"The bakery’s croissants were legendary. Every morning, the scent of butter drew a crowd. Mrs. Laurent, the baker, perfected each one. Customers claimed they were the best they’d ever tasted.\"]\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc  0\n",
      "tensor([[1.0000, 0.3666, 0.1207],\n",
      "        [0.3666, 1.0000, 0.3891],\n",
      "        [0.1207, 0.3891, 1.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc  1\n",
      "tensor([[1.0000, 0.0569, 0.0826],\n",
      "        [0.0569, 1.0000, 0.1767],\n",
      "        [0.0826, 0.1767, 1.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc  2\n",
      "tensor([[1.0000, 0.2322, 0.4890, 0.3625],\n",
      "        [0.2322, 1.0000, 0.1858, 0.2375],\n",
      "        [0.4890, 0.1858, 1.0000, 0.2475],\n",
      "        [0.3625, 0.2375, 0.2475, 1.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_node_list, sent_node_list, edge_data_list, sentId_nodeId_list = define_node_edge(test_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0,\n",
       "  0,\n",
       "  'Although he was very busy with his work, Peter had had enough of it.'): 0,\n",
       " (0, 0, 'He and his wife decided they needed a holiday.'): 1,\n",
       " (0,\n",
       "  0,\n",
       "  'They travelled to Spain because they loved the country very much.'): 2,\n",
       " (0, 1, 'A stray cat appeared in the garden one rainy day.'): 3,\n",
       " (0, 1, 'Sarah offered milk and a blanket.'): 4,\n",
       " (0, 1, 'It stayed, becoming part of her family.'): 5,\n",
       " (0, 2, 'The bakery’s croissants were legendary.'): 6,\n",
       " (0, 2, 'Every morning, the scent of butter drew a crowd.'): 7,\n",
       " (0, 2, 'Mrs. Laurent, the baker, perfected each one.'): 8,\n",
       " (0, 2, 'Customers claimed they were the best they’d ever tasted.'): 9}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_node_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {(10, 0): [{'type': 'word_sent', 'weight': 0.4611}],\n",
       "             (11, 2): [{'type': 'word_sent', 'weight': 0.2796}],\n",
       "             (12, 3): [{'type': 'word_sent', 'weight': 0.4608}],\n",
       "             (13, 4): [{'type': 'word_sent', 'weight': 0.1903}],\n",
       "             (14, 6): [{'type': 'word_sent', 'weight': 0.6856}],\n",
       "             (15, 8): [{'type': 'word_sent', 'weight': 0.284}],\n",
       "             (1, 0): [{'type': 'pronoun_antecedent', 'weight': 1},\n",
       "              {'type': 'similarity', 'weight': tensor(0.3666)}],\n",
       "             (2, 1): [{'type': 'pronoun_antecedent', 'weight': 1},\n",
       "              {'type': 'similarity', 'weight': tensor(0.3891)}],\n",
       "             (5, 3): [{'type': 'pronoun_antecedent', 'weight': 1}],\n",
       "             (5, 4): [{'type': 'pronoun_antecedent', 'weight': 1}],\n",
       "             (8, 6): [{'type': 'similarity', 'weight': tensor(0.4890)}],\n",
       "             (9, 6): [{'type': 'similarity', 'weight': tensor(0.3625)}]})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### doc1\n",
    "'''\n",
    "Although he was very busy with his work, Peter had had enough of it. n1,sen1\n",
    "He and his wife decided they needed a holiday. n2,sen2\n",
    "They travelled to Spain because they loved the country very much. n3,sen3\n",
    "'''\n",
    "####\n",
    "#### doc2\n",
    "'''\n",
    "A stray cat appeared in the garden one rainy day. n4,sen1\n",
    "Sarah offered milk and a blanket. n5,sen2\n",
    "It stayed, becoming part of her family. n6,sen3\n",
    "'''\n",
    "####\n",
    "#### doc3\n",
    "'''\n",
    "The bakery’s croissants were legendary. n7,sen1\n",
    "Every morning, the scent of butter drew a crowd. n8,sen2\n",
    "Mrs. Laurent, the baker, perfected each one. n9,sen3\n",
    "Customers claimed they were the best they’d ever tasted. n10,sen4\n",
    "'''\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_kws = extract_keywords(test_docs_list)\n",
    "\n",
    "for docs in docs_kws:\n",
    "     for doc in docs:\n",
    "          print(\"final result: \", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corfs = coref_resolve(test_docs_list)\n",
    "\n",
    "for docs in corfs:\n",
    "     for doc in docs:\n",
    "          print(\"doc final result: \")\n",
    "          for cluster in doc:\n",
    "               print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_map, sent_map, edges = define_node_edge(test_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"word map: \", word_map)\n",
    "print(\"sent_map: \", sent_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in word_map[0].items():\n",
    "     print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (k1,k2), value in edges[0].items():\n",
    "     for e in value:\n",
    "          print(f\"{k1}, {k2}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coreferee_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
