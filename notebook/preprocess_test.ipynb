{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load, sentence split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_preprocess_utils import load_jsonl,split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = load_jsonl('../data/preprocess_test_data.jsonl')\n",
    "print(f\"original_data, size = {len(load_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sentences = split_sentences(load_data)\n",
    "print(\"splitted sentence: \\n\")\n",
    "for i in range(0, len(load_data)):\n",
    "     print(f\"multi-doc samples {i + 1} has {len(doc_sentences[i])} docs.\")\n",
    "     for doc in doc_sentences[i]:\n",
    "          print(f\"sentence number: {len(doc)}\")\n",
    "          print(f\"document sentences: {doc}\")\n",
    "\n",
    "     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keyword extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_preprocess_utils import extract_keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "doc = \"\"\"\n",
    "          Supervised learning is the machine learning task of learning a function that\n",
    "          maps an input to an output based on example input-output pairs.[1] It infers a\n",
    "          function from labeled training data consisting of a set of training examples.[2]\n",
    "          In supervised learning, each example is a pair consisting of an input object\n",
    "          (typically a vector) and a desired output value (also called the supervisory signal).\n",
    "          A supervised learning algorithm analyzes the training data and produces an inferred function,\n",
    "          which can be used for mapping new examples. An optimal scenario will allow for the\n",
    "          algorithm to correctly determine the class labels for unseen instances. This requires\n",
    "          the learning algorithm to generalize from the training data to unseen situations in a\n",
    "          'reasonable' way (see inductive bias).\n",
    "          \"\"\"\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence keyword\n",
    "s1 = \"Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.\"\n",
    "s2 = \"I am a women.\"\n",
    "s3 = \"hei!\"\n",
    "kw1 = kw_model.extract_keywords(s1)\n",
    "kw2 = kw_model.extract_keywords(s2)\n",
    "kw3 = kw_model.extract_keywords(s3)\n",
    "\n",
    "print(f\"sentence keywords: \\n kw1: {kw1} \\n kw2: {kw2} \\n kw3: {kw3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stop word remove\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# 获取 spaCy 的停用词列表\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "kw1_noStopWord = [(word, score) for word, score in kw1 if word.lower() not in stop_words]\n",
    "kw2_noStopWord = [(word, score) for word, score in kw2 if word.lower() not in stop_words]\n",
    "kw3_noStopWord = [(word, score) for word, score in kw3 if word.lower() not in stop_words]\n",
    "print(f\"sentence keywords without stopwords: \\n kw1: {kw1_noStopWord} \\n kw2: {kw2_noStopWord} \\n kw3: {kw3_noStopWord}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_keyword_count(text, words_per_100=1, min_keywords=2, max_keywords=1100):\n",
    "     # 计算文本长度\n",
    "     text_length = len(text)\n",
    "     print(text_length)\n",
    "     # 按比例计算关键词数量\n",
    "     keyword_count = max(min_keywords, min(max_keywords, text_length // 4 * words_per_100))\n",
    "     \n",
    "     return keyword_count\n",
    "\n",
    "# 示例\n",
    "text = \"这是一段示例文本，用于演示如何根据文本长度动态调整关键词数量。\"\n",
    "keyword_count = calculate_keyword_count(text)\n",
    "print(f\"建议提取的关键词数量: {keyword_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coreference resolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: he(1), his(6), Peter(9), He(16), his(18)\n",
      "1: work(7), it(14)\n",
      "2: [He(16); wife(19)], they(21), They(26), they(31)\n",
      "3: wife(19), She(38)\n",
      "4: Spain(29), country(34)\n",
      "5: [Tom(41); Anny(43); John(45)], their(49)\n",
      "index: 49, ancetedent: [Tom, Anny, John]\n"
     ]
    }
   ],
   "source": [
    "## new method try\n",
    "import coreferee\n",
    "import spacy\n",
    "\n",
    "nlp_l = spacy.load('en_core_web_lg')\n",
    "nlp_l.add_pipe('coreferee')\n",
    "doc = nlp_l('Although he was very busy with his work, Peter had had enough of it. He and his wife decided they needed a holiday. They travelled to Spain because they loved the country very much. She also meet Tom, Anny and John, who are their previous colleges')\n",
    "# print(type(doc))\n",
    "doc._.coref_chains.print()\n",
    "# print(type(doc._.coref_chains[0]))\n",
    "\n",
    "# for chain in doc._.coref_chains:\n",
    "#      print(f\"current chain: {chain}. \\n its mentions are: \")\n",
    "#      for mention in chain:\n",
    "#           print(mention)\n",
    "          \n",
    "# print(doc._.coref_chains.resolve(doc[31]))\n",
    "\n",
    "# print(doc._.coref_chains[0].pretty_representation)\n",
    "ind = doc._.coref_chains[5][1][0]\n",
    "print(f\"index: {ind}, ancetedent: {doc._.coref_chains.resolve(doc[ind])}\") #  ancetedent: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = doc._.coref_chains[0]\n",
    "print(f\"chain 1 antecend: {chain.most_specific_mention_index}, token_id:{chain[chain.most_specific_mention_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country is in the sent: They travelled to Spain because they loved the country very much.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{doc[34]} is in the sent: {doc[34].sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## over-all test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 20:34:12,810 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu - SentenceTransformer.py:210\n",
      "2025-01-22 20:34:12,810 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2 - SentenceTransformer.py:218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.data_preprocess_utils import define_node_edge, extract_keywords, coref_resolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs_list = [[\n",
    "     [\"Although he was very busy with his work, Peter had had enough of it. He and his wife decided they needed a holiday. They travelled to Spain because they loved the country very much.\"],\n",
    "     [\"A stray cat appeared in the garden one rainy day. Sarah offered milk and a blanket. It stayed, becoming part of her family.\"],\n",
    "     [\"The bakery’s croissants were legendary. Every morning, the scent of butter drew a crowd. Mrs. Laurent, the baker, perfected each one. Customers claimed they were the best they’d ever tasted.\"]\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### doc1\n",
    "'''\n",
    "Although he was very busy with his work, Peter had had enough of it. n1,sen1\n",
    "He and his wife decided they needed a holiday. n2,sen2\n",
    "They travelled to Spain because they loved the country very much. n3,sen3\n",
    "'''\n",
    "####\n",
    "#### doc2\n",
    "'''\n",
    "A stray cat appeared in the garden one rainy day. n4,sen1\n",
    "Sarah offered milk and a blanket. n5,sen2\n",
    "It stayed, becoming part of her family. n6,sen3\n",
    "'''\n",
    "####\n",
    "#### doc3\n",
    "'''\n",
    "The bakery’s croissants were legendary. n7,sen1\n",
    "Every morning, the scent of butter drew a crowd. n8,sen2\n",
    "Mrs. Laurent, the baker, perfected each one. n9,sen3\n",
    "Customers claimed they were the best they’d ever tasted. n10,sen4\n",
    "'''\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 17:29:04,891 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu - SentenceTransformer.py:210\n",
      "2025-01-22 17:29:04,891 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2 - SentenceTransformer.py:218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:  [('peter', 0.4611), ('spain', 0.2796)]\n",
      "final result:  [('stray', 0.4608), ('milk', 0.1903)]\n",
      "final result:  [('croissants', 0.6856), ('laurent', 0.284)]\n"
     ]
    }
   ],
   "source": [
    "docs_kws = extract_keywords(test_docs_list)\n",
    "\n",
    "for docs in docs_kws:\n",
    "     for doc in docs:\n",
    "          print(\"final result: \", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 19:17:02,420 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu - SentenceTransformer.py:210\n",
      "2025-01-22 19:17:02,423 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2 - SentenceTransformer.py:218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain:  0: he(1), his(6), Peter(9), He(16), his(18)\n",
      "ant: 9: \n",
      " text: Although he was very busy with his work, Peter had had enough of it.\n",
      "chain:  1: work(7), it(14)\n",
      "ant: 7: \n",
      " text: Although he was very busy with his work, Peter had had enough of it.\n",
      "chain:  2: [He(16); wife(19)], they(21), They(26), they(31)\n",
      "ant: 16: \n",
      " text: He and his wife decided they needed a holiday.\n",
      "chain:  3: Spain(29), country(34)\n",
      "ant: 29: \n",
      " text: They travelled to Spain because they loved the country very much.\n",
      "chain:  0: cat(2), It(18)\n",
      "ant: 2: \n",
      " text: A stray cat appeared in the garden one rainy day.\n",
      "chain:  1: Sarah(11), her(24)\n",
      "ant: 11: \n",
      " text: Sarah offered milk and a blanket.\n",
      "chain:  0: Customers(28), they(30), they(34)\n",
      "ant: 28: \n",
      " text: Customers claimed they were the best they’d ever tasted.\n",
      "doc final result: \n",
      "[(0, 0, Although he was very busy with his work, Peter had had enough of it.), (0, 0, He and his wife decided they needed a holiday.)]\n",
      "[(0, 0, He and his wife decided they needed a holiday.), (0, 0, They travelled to Spain because they loved the country very much.)]\n",
      "doc final result: \n",
      "[(0, 1, A stray cat appeared in the garden one rainy day.), (0, 1, It stayed, becoming part of her family.)]\n",
      "[(0, 1, Sarah offered milk and a blanket.), (0, 1, It stayed, becoming part of her family.)]\n",
      "doc final result: \n"
     ]
    }
   ],
   "source": [
    "corfs = coref_resolve(test_docs_list)\n",
    "\n",
    "for docs in corfs:\n",
    "     for doc in docs:\n",
    "          print(\"doc final result: \")\n",
    "          for cluster in doc:\n",
    "               print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 20:57:28,982 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu - SentenceTransformer.py:210\n",
      "2025-01-22 20:57:29,011 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2 - SentenceTransformer.py:218\n"
     ]
    }
   ],
   "source": [
    "word_map, sent_map, edges = define_node_edge(test_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word map:  [{'peter': 10, 'spain': 11, 'stray': 12, 'milk': 13, 'croissants': 14, 'laurent': 15}]\n",
      "sent_map:  [{(0, 0, 'Although he was very busy with his work, Peter had had enough of it.'): 0, (0, 0, 'He and his wife decided they needed a holiday.'): 1, (0, 0, 'They travelled to Spain because they loved the country very much.'): 2, (0, 1, 'A stray cat appeared in the garden one rainy day.'): 3, (0, 1, 'Sarah offered milk and a blanket.'): 4, (0, 1, 'It stayed, becoming part of her family.'): 5, (0, 2, 'The bakery’s croissants were legendary.'): 6, (0, 2, 'Every morning, the scent of butter drew a crowd.'): 7, (0, 2, 'Mrs. Laurent, the baker, perfected each one.'): 8, (0, 2, 'Customers claimed they were the best they’d ever tasted.'): 9}]\n"
     ]
    }
   ],
   "source": [
    "print(\"word map: \", word_map)\n",
    "print(\"sent_map: \", sent_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peter: 10\n",
      "spain: 11\n",
      "stray: 12\n",
      "milk: 13\n",
      "croissants: 14\n",
      "laurent: 15\n"
     ]
    }
   ],
   "source": [
    "for key, value in word_map[0].items():\n",
    "     print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10, 0: {'type': 'word_sent', 'weight': 0.4611}\n",
      "11, 2: {'type': 'word_sent', 'weight': 0.2796}\n",
      "12, 3: {'type': 'word_sent', 'weight': 0.4608}\n",
      "13, 4: {'type': 'word_sent', 'weight': 0.1903}\n",
      "14, 6: {'type': 'word_sent', 'weight': 0.6856}\n",
      "15, 8: {'type': 'word_sent', 'weight': 0.284}\n",
      "1, 0: {'type': 'pronoun_antecedent', 'weight': 1}\n",
      "2, 1: {'type': 'pronoun_antecedent', 'weight': 1}\n",
      "5, 3: {'type': 'pronoun_antecedent', 'weight': 1}\n",
      "5, 4: {'type': 'pronoun_antecedent', 'weight': 1}\n"
     ]
    }
   ],
   "source": [
    "for (k1,k2), value in edges[0].items():\n",
    "     for e in value:\n",
    "          print(f\"{k1}, {k2}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coreferee_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
