{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\coreferee_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from models.gnn_train import train_gnn, get_gnn_trained_embedding\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: <bound method SummaryDataset.__len__ of <models.DatasetLoader.SummaryDataset object at 0x00000206A44C86D0>>\n",
      "hidden_size:128, out_size: 256, num_heads: 3, sentence_in_size: 768, word_in_size: 768, feat_drop: 0.2, attn_drop: 0.2\n",
      "Initial sentence_feat: tensor([[-0.0106, -0.0281, -0.0433,  ..., -0.0406,  0.0261,  0.0451],\n",
      "        [ 0.0117, -0.0213, -0.0567,  ..., -0.0164,  0.0444,  0.0274],\n",
      "        [-0.0309, -0.0361, -0.0391,  ..., -0.0118,  0.0344, -0.0021],\n",
      "        ...,\n",
      "        [-0.0304, -0.0248, -0.0056,  ..., -0.0141, -0.0391,  0.0263],\n",
      "        [-0.0316, -0.0353, -0.0104,  ..., -0.0025,  0.0190, -0.0063],\n",
      "        [-0.0047, -0.0168,  0.0017,  ..., -0.0401,  0.0083,  0.0074]])\n",
      "Initial word_feat: tensor([[-0.0071, -0.0108, -0.0136,  ..., -0.0095, -0.0129, -0.0297],\n",
      "        [-0.0180, -0.0373, -0.0034,  ...,  0.0090, -0.0046, -0.0020],\n",
      "        [ 0.0132, -0.0273, -0.0112,  ..., -0.0034,  0.0024, -0.0186],\n",
      "        [ 0.0120, -0.0411, -0.0159,  ..., -0.0181, -0.0033, -0.0133],\n",
      "        [-0.0049, -0.0108, -0.0134,  ..., -0.0165, -0.0292, -0.0028]])\n",
      "Initial h: {'sentence': tensor([[0.0129, 0.0059, 0.0000,  ..., 0.0154, 0.0000, 0.0236],\n",
      "        [0.0237, 0.0000, 0.0206,  ..., 0.0000, 0.0000, 0.0202],\n",
      "        [0.0089, 0.0063, 0.0056,  ..., 0.0129, 0.0062, 0.0130],\n",
      "        ...,\n",
      "        [0.0093, 0.0152, 0.0139,  ..., 0.0050, 0.0151, 0.0080],\n",
      "        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0201, 0.0100],\n",
      "        [0.0116, 0.0125, 0.0000,  ..., 0.0054, 0.0221, 0.0005]],\n",
      "       grad_fn=<ReluBackward0>), 'word': tensor([[0.0433, 0.0000, 0.0399,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0366, 0.0000, 0.0303,  ..., 0.0000, 0.0025, 0.0187],\n",
      "        [0.0393, 0.0000, 0.0363,  ..., 0.0048, 0.0076, 0.0000],\n",
      "        [0.0368, 0.0000, 0.0360,  ..., 0.0000, 0.0100, 0.0000],\n",
      "        [0.0350, 0.0000, 0.0422,  ..., 0.0000, 0.0056, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)}\n",
      "After conv1: {'sentence': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SumBackward1>), 'word': tensor([[ 1.1548e-02, -2.3277e-02, -7.7283e-03,  ..., -2.5412e-03,\n",
      "         -1.6121e-02,  4.9275e-03],\n",
      "        [ 1.9549e-02, -4.3024e-02, -3.4713e-02,  ..., -6.2878e-03,\n",
      "         -3.1907e-02,  3.0070e-02],\n",
      "        [ 1.2672e-02, -4.1366e-02, -8.5102e-03,  ..., -3.8351e-05,\n",
      "         -6.9225e-03,  3.7678e-03],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -8.1421e-03,\n",
      "         -2.8765e-02,  1.9728e-02],\n",
      "        [ 2.4852e-02, -2.2298e-02,  1.6491e-02,  ..., -3.1331e-03,\n",
      "         -2.6977e-02,  2.7198e-02]], grad_fn=<AddBackward0>)}\n",
      "After dropout: {'sentence': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<MulBackward0>), 'word': tensor([[ 1.4435e-02, -2.9096e-02, -9.6603e-03,  ..., -3.1765e-03,\n",
      "         -2.0151e-02,  6.1594e-03],\n",
      "        [ 2.4436e-02, -5.3779e-02, -4.3391e-02,  ..., -7.8598e-03,\n",
      "         -3.9884e-02,  3.7588e-02],\n",
      "        [ 1.5839e-02, -0.0000e+00, -1.0638e-02,  ..., -4.7939e-05,\n",
      "         -8.6532e-03,  4.7098e-03],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0178e-02,\n",
      "         -3.5956e-02,  2.4661e-02],\n",
      "        [ 0.0000e+00, -0.0000e+00,  2.0614e-02,  ..., -3.9164e-03,\n",
      "         -3.3721e-02,  0.0000e+00]], grad_fn=<MulBackward0>)}\n",
      "After flatten: {'sentence': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<MulBackward0>), 'word': tensor([[ 1.4435e-02, -2.9096e-02, -9.6603e-03,  ..., -3.1765e-03,\n",
      "         -2.0151e-02,  6.1594e-03],\n",
      "        [ 2.4436e-02, -5.3779e-02, -4.3391e-02,  ..., -7.8598e-03,\n",
      "         -3.9884e-02,  3.7588e-02],\n",
      "        [ 1.5839e-02, -0.0000e+00, -1.0638e-02,  ..., -4.7939e-05,\n",
      "         -8.6532e-03,  4.7098e-03],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0178e-02,\n",
      "         -3.5956e-02,  2.4661e-02],\n",
      "        [ 0.0000e+00, -0.0000e+00,  2.0614e-02,  ..., -3.9164e-03,\n",
      "         -3.3721e-02,  0.0000e+00]], grad_fn=<MulBackward0>)}\n",
      "After conv2: {'sentence': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SumBackward1>), 'word': tensor([[ 0.0307, -0.0531,  0.0730,  ...,  0.0224,  0.0027,  0.0012],\n",
      "        [ 0.0287, -0.0087,  0.0385,  ..., -0.0041,  0.0218, -0.0102],\n",
      "        [ 0.0647, -0.0595,  0.0470,  ..., -0.0276,  0.0054,  0.0138],\n",
      "        [ 0.0512, -0.0520,  0.0676,  ...,  0.0377,  0.0046, -0.0172],\n",
      "        [-0.0079, -0.0276,  0.0461,  ...,  0.0313,  0.0010,  0.0042]],\n",
      "       grad_fn=<AddBackward0>)}\n",
      "batch_embeddings size: torch.Size([66, 256])\n",
      "projected_embeddings size: torch.Size([66, 512])\n",
      "reshape_embeddings shape: torch.Size([66, 1, 512])\n",
      "reshape_embeddings: tensor([[[ 0.0496, -0.0087, -0.0510,  ...,  0.0419,  0.0500, -0.0499]],\n",
      "\n",
      "        [[ 0.0496, -0.0087, -0.0510,  ...,  0.0419,  0.0500, -0.0499]],\n",
      "\n",
      "        [[ 0.0496, -0.0087, -0.0510,  ...,  0.0419,  0.0500, -0.0499]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0496, -0.0087, -0.0510,  ...,  0.0419,  0.0500, -0.0499]],\n",
      "\n",
      "        [[ 0.0496, -0.0087, -0.0510,  ...,  0.0419,  0.0500, -0.0499]],\n",
      "\n",
      "        [[ 0.0496, -0.0087, -0.0510,  ...,  0.0419,  0.0500, -0.0499]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "closest_token_ids: size: torch.Size([66, 512]), data: tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(\"..\",\"data\", \"multinews\", \"test.jsonl\")\n",
    "train_gnn(\n",
    "     file_path = test_path,\n",
    "     hidden_size=128,\n",
    "     out_size=256,\n",
    "     num_heads=3,\n",
    "     learning_rate=0.001,\n",
    "     num_epochs=2,\n",
    "     batch_size=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coreferee_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
