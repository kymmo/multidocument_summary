{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph construct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs_list = [[\n",
    "     [\"Although he was very busy with his work, Peter had had enough of it. He and his wife decided they needed a holiday. They travelled to Spain because they loved the country very much.\"],\n",
    "     [\"A stray cat appeared in the garden one rainy day. Sarah offered milk and a blanket. It stayed, becoming part of her family.\"],\n",
    "     [\"The bakery’s croissants were legendary. Every morning, the scent of butter drew a crowd. Mrs. Laurent, the baker, perfected each one. Customers claimed they were the best they’d ever tasted.\"]\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\coreferee_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-22 21:32:05,768 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu - SentenceTransformer.py:210\n",
      "2025-01-22 21:32:05,794 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2 - SentenceTransformer.py:218\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.graph_utils import create_graph\n",
    "from utils.data_preprocess_utils import define_node_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_map, sent_map, edges = define_node_edge(test_docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list = create_graph(word_map, sent_map, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {(10, 0): [{'type': 'word_sent', 'weight': 0.4611}],\n",
       "             (11, 2): [{'type': 'word_sent', 'weight': 0.2796}],\n",
       "             (12, 3): [{'type': 'word_sent', 'weight': 0.4608}],\n",
       "             (13, 4): [{'type': 'word_sent', 'weight': 0.1903}],\n",
       "             (14, 6): [{'type': 'word_sent', 'weight': 0.6856}],\n",
       "             (15, 8): [{'type': 'word_sent', 'weight': 0.284}],\n",
       "             (1, 0): [{'type': 'pronoun_antecedent', 'weight': 1}],\n",
       "             (2, 1): [{'type': 'pronoun_antecedent', 'weight': 1}],\n",
       "             (5, 3): [{'type': 'pronoun_antecedent', 'weight': 1}],\n",
       "             (5, 4): [{'type': 'pronoun_antecedent', 'weight': 1}]})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes with data: [(10, {'type': 'word', 'text': 'peter'}), (11, {'type': 'word', 'text': 'spain'}), (12, {'type': 'word', 'text': 'stray'}), (13, {'type': 'word', 'text': 'milk'}), (14, {'type': 'word', 'text': 'croissants'}), (15, {'type': 'word', 'text': 'laurent'}), (0, {'type': 'sentence', 'text': (0, 0, 'Although he was very busy with his work, Peter had had enough of it.')}), (1, {'type': 'sentence', 'text': (0, 0, 'He and his wife decided they needed a holiday.')}), (2, {'type': 'sentence', 'text': (0, 0, 'They travelled to Spain because they loved the country very much.')}), (3, {'type': 'sentence', 'text': (0, 1, 'A stray cat appeared in the garden one rainy day.')}), (4, {'type': 'sentence', 'text': (0, 1, 'Sarah offered milk and a blanket.')}), (5, {'type': 'sentence', 'text': (0, 1, 'It stayed, becoming part of her family.')}), (6, {'type': 'sentence', 'text': (0, 2, 'The bakery’s croissants were legendary.')}), (7, {'type': 'sentence', 'text': (0, 2, 'Every morning, the scent of butter drew a crowd.')}), (8, {'type': 'sentence', 'text': (0, 2, 'Mrs. Laurent, the baker, perfected each one.')}), (9, {'type': 'sentence', 'text': (0, 2, 'Customers claimed they were the best they’d ever tasted.')})]\n"
     ]
    }
   ],
   "source": [
    "print(\"Nodes with data:\", graph_list[0].nodes(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges with data: [(10, 0, {'edge_type': 'word_sent', 'weight': 0.4611}), (11, 2, {'edge_type': 'word_sent', 'weight': 0.2796}), (12, 3, {'edge_type': 'word_sent', 'weight': 0.4608}), (13, 4, {'edge_type': 'word_sent', 'weight': 0.1903}), (14, 6, {'edge_type': 'word_sent', 'weight': 0.6856}), (15, 8, {'edge_type': 'word_sent', 'weight': 0.284}), (0, 10, {'edge_type': 'word_sent', 'weight': 0.4611}), (0, 1, {'edge_type': 'pronoun_antecedent', 'weight': 1}), (1, 0, {'edge_type': 'pronoun_antecedent', 'weight': 1}), (1, 2, {'edge_type': 'pronoun_antecedent', 'weight': 1}), (2, 11, {'edge_type': 'word_sent', 'weight': 0.2796}), (2, 1, {'edge_type': 'pronoun_antecedent', 'weight': 1}), (3, 12, {'edge_type': 'word_sent', 'weight': 0.4608}), (3, 5, {'edge_type': 'pronoun_antecedent', 'weight': 1}), (4, 13, {'edge_type': 'word_sent', 'weight': 0.1903}), (4, 5, {'edge_type': 'pronoun_antecedent', 'weight': 1}), (5, 3, {'edge_type': 'pronoun_antecedent', 'weight': 1}), (5, 4, {'edge_type': 'pronoun_antecedent', 'weight': 1}), (6, 14, {'edge_type': 'word_sent', 'weight': 0.6856}), (8, 15, {'edge_type': 'word_sent', 'weight': 0.284})]\n"
     ]
    }
   ],
   "source": [
    "print(\"Edges with data:\", graph_list[0].edges(data=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# 加载 BERT tokenizer 和模型\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入单词\n",
    "word = \"hello\"\n",
    "\n",
    "# 将单词转换为 token ID\n",
    "inputs = bert_tokenizer(word, return_tensors=\"pt\")  # 返回 PyTorch 张量\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# 打印 token ID\n",
    "print(\"Input IDs:\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 BERT 的 embedding 层\n",
    "embedding_layer = bert_model.embeddings.word_embeddings\n",
    "\n",
    "# 将 token ID 转换为 word embedding\n",
    "with torch.no_grad():\n",
    "     word_embedding = embedding_layer(input_ids)\n",
    "     \n",
    "# 如果单词被拆分为多个 subword tokens，取平均\n",
    "if word_embedding.shape[1] > 1:\n",
    "     word_embedding = word_embedding.mean(dim=1)  # 沿 sequence_length 维度取平均\n",
    "\n",
    "\n",
    "# 打印 word embedding 的形状\n",
    "print(\"Word Embedding Shape:\", word_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similarity edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph embedding combine test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# 假设 GNN 模型已经定义并生成 embedding\n",
    "class GNNModel(nn.Module):\n",
    "     def __init__(self, input_size, hidden_size):\n",
    "          super().__init__()\n",
    "          self.fc = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "     def forward(self, graph_input):\n",
    "          # 这里假设 graph_input 是图的表示，例如节点特征\n",
    "          # 返回 GNN 生成的 embedding\n",
    "          return self.fc(graph_input)\n",
    "\n",
    "# 初始化 GNN 模型\n",
    "gnn_model = GNNModel(input_size=128, hidden_size=256)  # 假设 GNN 的 hidden size 是 256\n",
    "\n",
    "# 初始化 T5 模型和 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# 假设输入文本和图的表示\n",
    "input_text = \"This is an example sentence.\"\n",
    "graph_input = torch.randn(1, 10, 128)  # [batch_size, seq_len, gnn_input_size]\n",
    "\n",
    "# 1. 使用 GNN 生成 embedding\n",
    "gnn_embeddings = gnn_model(graph_input)  # [batch_size, seq_len, gnn_hidden_size]\n",
    "\n",
    "# 2. 使用 T5 tokenizer 对输入文本进行编码\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids  # [batch_size, seq_len]\n",
    "\n",
    "# 3. 获取 T5 的 token embedding\n",
    "token_embeddings = t5_model.get_input_embeddings()(input_ids)  # [batch_size, seq_len, t5_hidden_size]\n",
    "\n",
    "# 4. 将 GNN embedding 和 T5 token embedding 拼接\n",
    "# 首先需要将 GNN embedding 的序列长度与 T5 token embedding 对齐\n",
    "# 这里假设 GNN embedding 的序列长度与 T5 token embedding 相同\n",
    "# 如果不同，可以通过插值或截断对齐\n",
    "combined_embeddings = torch.cat([token_embeddings, gnn_embeddings], dim=-1)  # [batch_size, seq_len, t5_hidden_size + gnn_hidden_size]\n",
    "\n",
    "# 5. 通过线性变换将拼接后的 embedding 映射到 T5 的 hidden size\n",
    "linear_layer = nn.Linear(t5_model.config.hidden_size + gnn_embeddings.size(-1), t5_model.config.hidden_size)\n",
    "mapped_embeddings = linear_layer(combined_embeddings)  # [batch_size, seq_len, t5_hidden_size]\n",
    "\n",
    "# 6. 使用 inputs_embeds 参数传入自定义 embedding\n",
    "outputs = t5_model(inputs_embeds=mapped_embeddings)\n",
    "\n",
    "# 7. 获取生成的输出\n",
    "generated_ids = t5_model.generate(inputs_embeds=mapped_embeddings)\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Summary:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 加载 T5 tokenizer 和模型\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 输入文本\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\coreferee_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1690\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1690\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\anaconda\\envs\\coreferee_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1678\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1676\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1678\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "## get embedding\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# 加载 T5 tokenizer 和模型\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# 输入文本\n",
    "text = \"Translate English to French: The house is wonderful.\"\n",
    "\n",
    "# 将文本转换为 token ID 序列\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 使用 embedding 层生成词向量\n",
    "with torch.no_grad():\n",
    "     inputs_embeds = model.get_input_embeddings()(input_ids) ##  inputs_embeds = model.encoder.embed_tokens(input_ids)\n",
    "\n",
    "# 打印词向量的形状\n",
    "print(\"Inputs Embeds Shape:\", inputs_embeds.shape)\n",
    "\n",
    "# 打印词向量\n",
    "print(\"Inputs Embeds:\", inputs_embeds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coreferee_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
